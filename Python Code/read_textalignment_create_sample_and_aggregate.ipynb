{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from legiscan import LegiScan\n",
    "from legiscan import LegiScanError\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "import gzip\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./Data/alignments_notext.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 10000000  #\n",
    "\n",
    "\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    '''  This function reads the text alignment data chunk by chunk.\n",
    "       For each chunk, we group by the state of the left and right bill to find the \n",
    "          state pair's aggregate alignment scores. We update the average alignment score for each chunk \n",
    "             read and grouped   '''\n",
    "\n",
    "    # Create new columns for left and right state by slicing the first two characters of 'left_id' and 'right_id'\n",
    "    chunk['left_state'] = chunk['left_id'].str[:2]\n",
    "    chunk['right_state'] = chunk['right_id'].str[:2]\n",
    "    \n",
    "    # Group by the state pairs and calculate mean, sum, and median for both 'score' and 'adjusted_alignment_score'\n",
    "    grouped = chunk.groupby(['left_state', 'right_state']).agg({\n",
    "        'score': ['mean', 'sum', 'median'],\n",
    "        'adjusted_alignment_score': ['mean', 'sum', 'median']\n",
    "    }).reset_index()\n",
    "    # Flatten MultiIndex columns after groupby aggregation\n",
    "    grouped.columns = ['_'.join(col).strip() if col[1] else col[0] for col in grouped.columns.values]\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "#initialize empty dataframe\n",
    "aggregated_df = pd.DataFrame()\n",
    "\n",
    "#begin processing chunks\n",
    "i = 0\n",
    "for chunk in pd.read_csv(file_path, usecols=['left_id', 'right_id', 'score', 'adjusted_alignment_score'], chunksize=chunk_size):\n",
    "    aggregated_chunk = process_chunk(chunk)\n",
    "    aggregated_df = pd.concat([aggregated_df, aggregated_chunk], ignore_index=True)\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "\n",
    "# Final aggregation\n",
    "final_results = aggregated_df.groupby(['left_state', 'right_state']).agg({\n",
    "    'score_mean': 'mean',\n",
    "    'score_sum': 'sum',\n",
    "    'score_median': 'median',\n",
    "    'adjusted_alignment_score_mean': 'mean',\n",
    "    'adjusted_alignment_score_sum': 'sum',\n",
    "    'adjusted_alignment_score_median': 'median'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_results.to_excel('./Data/aggregate_alignments.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_state</th>\n",
       "      <th>right_state</th>\n",
       "      <th>score_mean</th>\n",
       "      <th>score_sum</th>\n",
       "      <th>score_median</th>\n",
       "      <th>adjusted_alignment_score_mean</th>\n",
       "      <th>adjusted_alignment_score_sum</th>\n",
       "      <th>adjusted_alignment_score_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ak</td>\n",
       "      <td>al</td>\n",
       "      <td>22.410536</td>\n",
       "      <td>108367.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>12.867313</td>\n",
       "      <td>62151.8949</td>\n",
       "      <td>10.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ak</td>\n",
       "      <td>ar</td>\n",
       "      <td>22.692757</td>\n",
       "      <td>63621.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>10.757733</td>\n",
       "      <td>30178.1415</td>\n",
       "      <td>8.8880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  left_state right_state  score_mean  score_sum  score_median  \\\n",
       "0         ak          al   22.410536   108367.0          21.0   \n",
       "1         ak          ar   22.692757    63621.0          22.0   \n",
       "\n",
       "   adjusted_alignment_score_mean  adjusted_alignment_score_sum  \\\n",
       "0                      12.867313                    62151.8949   \n",
       "1                      10.757733                    30178.1415   \n",
       "\n",
       "   adjusted_alignment_score_median  \n",
       "0                          10.4475  \n",
       "1                           8.8880  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    ''' This function is for the policy coverage by each state.\n",
    "     We read the raw bills data by each line, corresponding to each document\n",
    "    If bill text is available for the document we append state and year for the bill in a list \n",
    "    Then convert the lists into a dataframe '''\n",
    "\n",
    "    year_col = []\n",
    "    state_col = []\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        line_count = 0  # Initialize line counter\n",
    "        for line in file:\n",
    "            try:\n",
    "                bill = json.loads(line.strip())\n",
    "                if (bill.get('bill_document_first') is None) and (bill.get('bill_document_last') is None):\n",
    "                    continue\n",
    "\n",
    "                # Convert 'date_created' to datetime object and check if within range\n",
    "                date_created_str = bill.get('date_created')\n",
    "                if date_created_str:\n",
    "                    date_created = datetime.strptime(date_created_str, '%Y-%m-%d %H:%M:%S')\n",
    "                    year = date_created.year\n",
    "                else:\n",
    "                    year = np.nan\n",
    "\n",
    "                state = bill.get('state')\n",
    "                if state:\n",
    "                    state_name = state\n",
    "                else:\n",
    "                    state_name = np.nan\n",
    "\n",
    "                year_col.append(year)\n",
    "                state_col.append(state_name)\n",
    "                line_count += 1\n",
    "                if line_count % 10000 == 0: \n",
    "                    print(line_count)\n",
    "                    end = time.time()\n",
    "                    time_taken = (end - start) / 60\n",
    "                    print('Time taken to process 10000 lines %s minutes' % time_taken)\n",
    "                    start = time.time()\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "    state_year = pd.DataFrame({'state': state_col, 'year': year_col})\n",
    "\n",
    "    return state_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Time taken to process 10000 lines 0.013874638080596923 minutes\n",
      "20000\n",
      "Time taken to process 10000 lines 0.015050045649210612 minutes\n",
      "30000\n",
      "Time taken to process 10000 lines 0.0157191793123881 minutes\n",
      "40000\n",
      "Time taken to process 10000 lines 0.014189803600311279 minutes\n",
      "50000\n",
      "Time taken to process 10000 lines 0.013550794124603272 minutes\n",
      "60000\n",
      "Time taken to process 10000 lines 0.01651686429977417 minutes\n",
      "70000\n",
      "Time taken to process 10000 lines 0.02893095016479492 minutes\n",
      "80000\n",
      "Time taken to process 10000 lines 0.01797702709833781 minutes\n",
      "90000\n",
      "Time taken to process 10000 lines 0.015256174405415853 minutes\n",
      "100000\n",
      "Time taken to process 10000 lines 0.017199369271596272 minutes\n",
      "110000\n",
      "Time taken to process 10000 lines 0.01769241491953532 minutes\n",
      "120000\n",
      "Time taken to process 10000 lines 0.01642011006673177 minutes\n",
      "130000\n",
      "Time taken to process 10000 lines 0.0163647452990214 minutes\n",
      "140000\n",
      "Time taken to process 10000 lines 0.01787092685699463 minutes\n",
      "150000\n",
      "Time taken to process 10000 lines 0.015078111489613851 minutes\n",
      "160000\n",
      "Time taken to process 10000 lines 0.015002787113189697 minutes\n",
      "170000\n",
      "Time taken to process 10000 lines 0.01721872091293335 minutes\n",
      "180000\n",
      "Time taken to process 10000 lines 0.02008885939915975 minutes\n",
      "190000\n",
      "Time taken to process 10000 lines 0.031058422724405923 minutes\n",
      "200000\n",
      "Time taken to process 10000 lines 0.02335426410039266 minutes\n",
      "210000\n",
      "Time taken to process 10000 lines 0.022469802697499593 minutes\n",
      "220000\n",
      "Time taken to process 10000 lines 0.02709801197052002 minutes\n",
      "230000\n",
      "Time taken to process 10000 lines 0.020357024669647217 minutes\n",
      "240000\n",
      "Time taken to process 10000 lines 0.019642802079518636 minutes\n",
      "250000\n",
      "Time taken to process 10000 lines 0.020354092121124268 minutes\n",
      "260000\n",
      "Time taken to process 10000 lines 0.015033976236979166 minutes\n",
      "270000\n",
      "Time taken to process 10000 lines 0.011468203862508138 minutes\n",
      "280000\n",
      "Time taken to process 10000 lines 0.016822608311971028 minutes\n",
      "290000\n",
      "Time taken to process 10000 lines 0.016863425572713215 minutes\n",
      "300000\n",
      "Time taken to process 10000 lines 0.020023258527119954 minutes\n",
      "310000\n",
      "Time taken to process 10000 lines 0.016349995136260988 minutes\n",
      "320000\n",
      "Time taken to process 10000 lines 0.01695310672124227 minutes\n",
      "330000\n",
      "Time taken to process 10000 lines 0.01619554360707601 minutes\n",
      "340000\n",
      "Time taken to process 10000 lines 0.015346447626749674 minutes\n",
      "350000\n",
      "Time taken to process 10000 lines 0.013705039024353027 minutes\n",
      "360000\n",
      "Time taken to process 10000 lines 0.013682723045349121 minutes\n",
      "370000\n",
      "Time taken to process 10000 lines 0.017688175042470295 minutes\n",
      "380000\n",
      "Time taken to process 10000 lines 0.017313623428344728 minutes\n",
      "390000\n",
      "Time taken to process 10000 lines 0.020996121565500896 minutes\n",
      "400000\n",
      "Time taken to process 10000 lines 0.026428929964701333 minutes\n",
      "410000\n",
      "Time taken to process 10000 lines 0.028508432706197104 minutes\n",
      "420000\n",
      "Time taken to process 10000 lines 0.025724124908447266 minutes\n",
      "430000\n",
      "Time taken to process 10000 lines 0.018159906069437664 minutes\n",
      "440000\n",
      "Time taken to process 10000 lines 0.014560914039611817 minutes\n",
      "450000\n",
      "Time taken to process 10000 lines 0.024821356932322184 minutes\n",
      "460000\n",
      "Time taken to process 10000 lines 0.014400100708007813 minutes\n",
      "470000\n",
      "Time taken to process 10000 lines 0.011421453952789307 minutes\n",
      "480000\n",
      "Time taken to process 10000 lines 0.016780598958333334 minutes\n",
      "490000\n",
      "Time taken to process 10000 lines 0.02110337416330973 minutes\n",
      "500000\n",
      "Time taken to process 10000 lines 0.01850273609161377 minutes\n",
      "510000\n",
      "Time taken to process 10000 lines 0.019127591451009115 minutes\n"
     ]
    }
   ],
   "source": [
    "state_year = read_json('./Data/state_bills.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by state and find counts for each state. count column gives us policy coverage\n",
    "grouped_count = state_year.groupby(['state', 'year']).size().reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ak</td>\n",
       "      <td>2011</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ak</td>\n",
       "      <td>2012</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  year  count\n",
       "0    ak  2011    403\n",
       "1    ak  2012    232"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_count.to_excel('./Data/grouped_state_year.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbreviations = ['tn', 'nh', 'wi', 'md', 'sc', 'dc', 'ak', 'nm', 'ar', 'mt', 'ut',\n",
    "       'sd', 'la', 'il', 'hi', 'or', 'vt', 'in', 'ok', 'ks', 'oh', 'nc',\n",
    "       'ct', 'fl', 'az', 'mi', 'de', 'nd', 'pr', 'mn', 'ga', 'va', 'me',\n",
    "       'wa', 'ne', 'wv', 'ny', 'al', 'nj', 'tx', 'ca', 'ky', 'wy', 'pa',\n",
    "       'nv', 'ia', 'ma', 'id', 'ms', 'ri', 'mo']  \n",
    "\n",
    "# years = range(2008, 2016)  \n",
    "desired_samples_per_state_year = 10\n",
    "\n",
    "# Initialize a dictionary to track the count of samples for each state-year pair\n",
    "\n",
    "## different states have different years for which bill data is available. If we do not have appropriate state year pairs then we would have to loop through the entire dataframe.\n",
    "## This allows us to break early\n",
    "\n",
    "samples_count = {}\n",
    "for state in state_abbreviations:\n",
    "    for year in grouped_count[grouped_count.state == state].year.unique():\n",
    "        samples_count[(state, str(year))] = 0\n",
    "                      \n",
    "samples_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function to update our samples\n",
    "def update_samples(chunk, samples_count, samples_df):\n",
    "    ''' This function reads chunks of the text alignments data and samples for state year pair.\n",
    "    If the required sample for any or all states is not satsfied in this chunk move to next chunk and continue sampling '''\n",
    "    for state in state_abbreviations:\n",
    "        for year in grouped_count[grouped_count.state == state].year.unique():\n",
    "            key = (state, str(year))\n",
    "            if samples_count[key]<10:\n",
    "            # Filter rows based on state and year\n",
    "                filtered_chunk = chunk[chunk['left_id'].str.startswith(f\"{state}_{year}\")]\n",
    "                remaining_samples = desired_samples_per_state_year - samples_count[key]\n",
    "                if remaining_samples > 0:  # Need more samples\n",
    "                    sampled_rows = filtered_chunk.sample(min(len(filtered_chunk), remaining_samples), random_state=1)  # Change seed if needed\n",
    "                    samples_df = pd.concat([samples_df, sampled_rows], ignore_index=True)\n",
    "                    samples_count[key] += len(sampled_rows)\n",
    "    return samples_count, samples_df\n",
    "\n",
    "# File reading and sampling\n",
    "chunk_size = 1000000  # Adjust based on your memory constraints\n",
    "i = 0\n",
    "for chunk in pd.read_csv(file_path,usecols = ['left_id','right_id', 'score','adjusted_alignment_score'], chunksize=chunk_size):\n",
    "    samples_count, samples_df = update_samples(chunk, samples_count, samples_df)\n",
    "    samples_df.shape\n",
    "    i += 1\n",
    "    print(i)\n",
    "    jj = 0 \n",
    "    print(samples_count)\n",
    "    for m in samples_count.keys():\n",
    "        if samples_count[m]<10:\n",
    "            jj +=1\n",
    "    print ('Less then required sample for state unique pairs remaining ' + str(jj))\n",
    "    if all(count >= desired_samples_per_state_year for count in samples_count.values()):\n",
    "        break  # Exit loop if we've collected enough samples for each state-year combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract bill ids \n",
    "samples_df['left_bill_state'] = samples_df['left_id'].str[:2]\n",
    "samples_df['left_bill_id'] = samples_df['left_id'].str.split('_').str[-1]\n",
    "samples_df['right_bill_state'] = samples_df['right_id'].str[:2]\n",
    "samples_df['right_bill_id'] = samples_df['right_id'].str.split('_').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1480, 8)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.to_excel('./Data/sampled.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionary of unique left and right bills and their state. We use the ouptut to read only these samples from our raw bills data when finding bills whose sponsor names are needed \n",
    "dictionary_df = samples_df[['left_bill_id','left_bill_state']].drop_duplicates(keep = 'first')\n",
    "dictionary_df.reset_index(drop = True, inplace = True)\n",
    "left_bill_dictionary = {}\n",
    "for i in range(len(dictionary_df)):\n",
    "    left_bill_dictionary[dictionary_df['left_bill_id'][i]] = dictionary_df['left_bill_state'][i]\n",
    "\n",
    "dictionary_df2 = samples_df[['right_bill_id','right_bill_state']].drop_duplicates(keep = 'first')\n",
    "dictionary_df2.reset_index(drop = True, inplace = True)\n",
    "right_bill_dictionary = {}\n",
    "for i in range(len(dictionary_df)):\n",
    "    right_bill_dictionary[dictionary_df2['right_bill_id'][i]] = dictionary_df2['right_bill_state'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./Data/left_dict.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(left_bill_dictionary, pickle_file)\n",
    "\n",
    "with open('./Data/right_dict.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(right_bill_dictionary, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
